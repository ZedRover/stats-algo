{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import datasets ## imports datasets from scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = datasets.load_boston() ## loads Boston dataset from datasets library \n",
    "\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "target = pd.DataFrame(data.target, columns=[\"MEDV\"])\n",
    "# Concatenate y in the dataframe\n",
    "df_target = pd.concat([df,target], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. When variables are centered, the intercept term is always 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>MEDV</td>       <th>  R-squared:         </th> <td>   0.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   203.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Mar 2021</td> <th>  Prob (F-statistic):</th> <td>1.93e-86</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:52:47</td>     <th>  Log-Likelihood:    </th> <td> -1638.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3285.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   502</td>      <th>  BIC:               </th> <td>   3302.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td> 3.617e-14</td> <td>    0.275</td> <td> 1.31e-13</td> <td> 1.000</td> <td>   -0.541</td> <td>    0.541</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAS</th>      <td>    5.0278</td> <td>    1.097</td> <td>    4.585</td> <td> 0.000</td> <td>    2.873</td> <td>    7.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>        <td>    8.1855</td> <td>    0.407</td> <td>   20.119</td> <td> 0.000</td> <td>    7.386</td> <td>    8.985</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>       <td>   -0.0780</td> <td>    0.010</td> <td>   -7.684</td> <td> 0.000</td> <td>   -0.098</td> <td>   -0.058</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>179.098</td> <th>  Durbin-Watson:     </th> <td>   0.774</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1274.618</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.355</td>  <th>  Prob(JB):          </th> <td>1.66e-277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>10.287</td>  <th>  Cond. No.          </th> <td>    112.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   MEDV   R-squared:                       0.549\n",
       "Model:                            OLS   Adj. R-squared:                  0.546\n",
       "Method:                 Least Squares   F-statistic:                     203.8\n",
       "Date:                Tue, 30 Mar 2021   Prob (F-statistic):           1.93e-86\n",
       "Time:                        11:52:47   Log-Likelihood:                -1638.7\n",
       "No. Observations:                 506   AIC:                             3285.\n",
       "Df Residuals:                     502   BIC:                             3302.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept   3.617e-14      0.275   1.31e-13      1.000      -0.541       0.541\n",
       "CHAS           5.0278      1.097      4.585      0.000       2.873       7.182\n",
       "RM             8.1855      0.407     20.119      0.000       7.386       8.985\n",
       "AGE           -0.0780      0.010     -7.684      0.000      -0.098      -0.058\n",
       "==============================================================================\n",
       "Omnibus:                      179.098   Durbin-Watson:                   0.774\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1274.618\n",
       "Skew:                           1.355   Prob(JB):                    1.66e-277\n",
       "Kurtosis:                      10.287   Cond. No.                         112.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is to show that when all variables are centered, the intercept beta_0 is always 0.\n",
    "df_centered = df_target - df_target.mean(axis=0)\n",
    "model = smf.ols(formula='MEDV ~ CHAS + RM + AGE', data=df_centered).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of data [0.5 0.5] variance of data [0.25 0.25]\n",
      "scaled data is\n",
      " [[-1. -1.]\n",
      " [-1. -1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]]\n",
      "[[3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data = pd.DataFrame([[0, 0], [0, 0], [1, 1], [1, 1]],dtype=np.float64)\n",
    "scaler = StandardScaler() # call the scaler function\n",
    "scaler.fit(data) # use the scaler to fit the data\n",
    "print('mean of data', scaler.mean_, 'variance of data', scaler.var_)\n",
    "\n",
    "data_scaled = scaler.transform(data) # use transform to obtain the scaled data\n",
    "print('scaled data is\\n', data_scaled)\n",
    "\n",
    "print(scaler.transform([[2, 2]]))   # Now we can transform any data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso and Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Without Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Lasso (use sklearn package) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective for Lasso is: \n",
    "(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenghua/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/chenghua/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:9: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# First step, transformation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "y = target\n",
    "df_dummy = pd.get_dummies(df, columns = ['CHAS'],drop_first = True) # Change categorical to one-hot\n",
    "X = df_dummy.drop(columns=['ZN','INDUS','NOX','RAD','AGE','PTRATIO','B']) # we don't include these variables in the model\n",
    "scaler_x, scaler_y = StandardScaler(), StandardScaler()\n",
    "scaler_x.fit(X)\n",
    "scaler_y.fit(y)\n",
    "X = scaler_x.transform(X)\n",
    "y = scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.15, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso\n",
    "from sklearn.linear_model import Lasso      # Runs Lasso with a given parameter\n",
    "model = Lasso(alpha=0.15)                    # Here alpha is like lambda in our slide\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.          0.29736325 -0.         -0.01606816 -0.39639773  0.        ]\n",
      "[-6.09665115e-16]\n",
      "0.6131825436772258\n",
      "[0.56007244 0.2688365  0.87643327 0.85968643 0.79000964]\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_)         # beta_1, beta_2,...,beta_6\n",
    "print(model.intercept_)    # beta_0 (we see that with the transformation, beta_0 = 0)\n",
    "print(model.score(X,y))    # R^2\n",
    "y_pred = model.predict(X)  # Predicting y given X\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Ridge (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective for ridge is: \n",
    "||y - Xw||^2_2 + alpha * ||w||^2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=151.79999999999998, copy_X=True, fit_intercept=True,\n",
       "   max_iter=None, normalize=False, random_state=None, solver='auto',\n",
       "   tol=0.001)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge\n",
    "from sklearn.linear_model import Ridge      # Runs Ridge with a given parameter\n",
    "model = Ridge(alpha=0.15*(2*len(y)))                   # Here alpha is like lambda in our slide\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07711512  0.33139771 -0.0628562  -0.12809425 -0.35081919  0.08538305]]\n",
      "[-6.02842017e-16]\n",
      "0.6583909160735733\n",
      "[[0.60008944]\n",
      " [0.33749744]\n",
      " [0.94949148]\n",
      " [0.89725359]\n",
      " [0.84974177]]\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_)         # beta_1, beta_2,...,beta_6\n",
    "print(model.intercept_)    # beta_0\n",
    "print(model.score(X,y))    # R^2\n",
    "y_pred = model.predict(X)\n",
    "y_pred = model.predict(X)  # Predicting y given X\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Lasso and Ridge using statsmodels package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.674</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   175.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 30 Mar 2021</td> <th>  Prob (F-statistic):</th>          <td>1.87e-119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:57:21</td>     <th>  Log-Likelihood:    </th>          <td> -431.62</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th>          <td>   875.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   500</td>      <th>  BIC:               </th>          <td>   900.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>   -0.0583</td> <td>    0.032</td> <td>   -1.819</td> <td> 0.069</td> <td>   -0.121</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td>    0.3715</td> <td>    0.033</td> <td>   11.387</td> <td> 0.000</td> <td>    0.307</td> <td>    0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td>   -0.1536</td> <td>    0.032</td> <td>   -4.789</td> <td> 0.000</td> <td>   -0.217</td> <td>   -0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th> <td>   -0.1467</td> <td>    0.036</td> <td>   -4.111</td> <td> 0.000</td> <td>   -0.217</td> <td>   -0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th> <td>   -0.4746</td> <td>    0.039</td> <td>  -12.061</td> <td> 0.000</td> <td>   -0.552</td> <td>   -0.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th> <td>    0.0920</td> <td>    0.026</td> <td>    3.564</td> <td> 0.000</td> <td>    0.041</td> <td>    0.143</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>165.592</td> <th>  Durbin-Watson:     </th> <td>   0.934</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 648.809</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.446</td>  <th>  Prob(JB):          </th> <td>1.30e-141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 7.734</td>  <th>  Cond. No.          </th> <td>    3.10</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                      y   R-squared (uncentered):                   0.678\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.674\n",
       "Method:                 Least Squares   F-statistic:                              175.1\n",
       "Date:                Tue, 30 Mar 2021   Prob (F-statistic):                   1.87e-119\n",
       "Time:                        11:57:21   Log-Likelihood:                         -431.62\n",
       "No. Observations:                 506   AIC:                                      875.2\n",
       "Df Residuals:                     500   BIC:                                      900.6\n",
       "Df Model:                           6                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            -0.0583      0.032     -1.819      0.069      -0.121       0.005\n",
       "x2             0.3715      0.033     11.387      0.000       0.307       0.436\n",
       "x3            -0.1536      0.032     -4.789      0.000      -0.217      -0.091\n",
       "x4            -0.1467      0.036     -4.111      0.000      -0.217      -0.077\n",
       "x5            -0.4746      0.039    -12.061      0.000      -0.552      -0.397\n",
       "x6             0.0920      0.026      3.564      0.000       0.041       0.143\n",
       "==============================================================================\n",
       "Omnibus:                      165.592   Durbin-Watson:                   0.934\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              648.809\n",
       "Skew:                           1.446   Prob(JB):                    1.30e-141\n",
       "Kurtosis:                       7.734   Cond. No.                         3.10\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "model = sm.OLS(y, X).fit() ## sm.OLS(output, input)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective for this package is:\n",
    "    0.5∗𝑅𝑆𝑆/𝑛+𝑎𝑙𝑝ℎ𝑎∗((1−𝐿1_𝑤𝑡)∗|𝑝𝑎𝑟𝑎𝑚𝑠|22/2+𝐿1_𝑤𝑡∗|𝑝𝑎𝑟𝑎𝑚𝑠|1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.29734871  0.         -0.01604702 -0.39641816  0.        ]\n"
     ]
    }
   ],
   "source": [
    "model = sm.OLS(y, X).fit_regularized(alpha=0.15, L1_wt=1.0) # Lasso\n",
    "print(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07711512  0.33139771 -0.0628562  -0.12809425 -0.35081919  0.08538305]\n"
     ]
    }
   ],
   "source": [
    "model = sm.OLS(y, X).fit_regularized(alpha=0.15*(2), L1_wt=0)  # Ridge (Need to pay attention to the objective)\n",
    "print(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lasso and Ridge with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Split data in Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155\n",
      " 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173\n",
      " 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191\n",
      " 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209\n",
      " 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227\n",
      " 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245\n",
      " 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263\n",
      " 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281\n",
      " 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299\n",
      " 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
      " 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335\n",
      " 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353\n",
      " 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371\n",
      " 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389\n",
      " 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407\n",
      " 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425\n",
      " 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443\n",
      " 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461\n",
      " 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479\n",
      " 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497\n",
      " 498 499 500 501 502 503 504 505] TEST: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 203 204 205 206 207 208\n",
      " 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226\n",
      " 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244\n",
      " 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262\n",
      " 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280\n",
      " 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298\n",
      " 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334\n",
      " 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352\n",
      " 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370\n",
      " 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388\n",
      " 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406\n",
      " 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424\n",
      " 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442\n",
      " 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460\n",
      " 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478\n",
      " 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496\n",
      " 497 498 499 500 501 502 503 504 505] TEST: [102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155\n",
      " 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173\n",
      " 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191\n",
      " 192 193 194 195 196 197 198 199 200 201 202]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334\n",
      " 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352\n",
      " 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370\n",
      " 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388\n",
      " 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406\n",
      " 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424\n",
      " 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442\n",
      " 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460\n",
      " 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478\n",
      " 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496\n",
      " 497 498 499 500 501 502 503 504 505] TEST: [203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220\n",
      " 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n",
      " 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274\n",
      " 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292\n",
      " 293 294 295 296 297 298 299 300 301 302 303]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 405 406\n",
      " 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424\n",
      " 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442\n",
      " 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460\n",
      " 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478\n",
      " 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496\n",
      " 497 498 499 500 501 502 503 504 505] TEST: [304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321\n",
      " 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339\n",
      " 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357\n",
      " 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375\n",
      " 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393\n",
      " 394 395 396 397 398 399 400 401 402 403 404]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404] TEST: [405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422\n",
      " 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440\n",
      " 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458\n",
      " 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476\n",
      " 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494\n",
      " 495 496 497 498 499 500 501 502 503 504 505]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  0   1   2   4   5   7   8  10  12  15  16  17  19  20  22  24  26  27\n",
      "  28  29  30  31  33  36  37  39  40  42  43  44  46  48  51  52  53  54\n",
      "  55  56  57  59  60  61  62  66  67  68  69  71  72  73  74  75  76  77\n",
      "  78  79  80  81  82  83  85  86  87  88  90  91  92  93  94  95  97  98\n",
      "  99 100 102 103 104 105 106 107 108 109 110 112 113 114 116 117 118 119\n",
      " 120 121 122 124 125 126 127 128 129 131 132 133 135 136 137 138 139 141\n",
      " 143 145 146 147 148 149 150 151 152 153 154 155 157 158 159 162 164 165\n",
      " 166 167 168 169 170 171 173 174 176 177 178 179 180 181 182 183 184 185\n",
      " 186 187 188 189 190 192 194 195 196 198 199 200 201 202 203 204 206 207\n",
      " 208 209 210 211 212 213 214 216 217 218 219 220 221 222 224 225 226 227\n",
      " 228 229 231 232 233 234 235 237 238 239 240 241 243 244 245 248 249 250\n",
      " 251 252 253 254 255 256 257 258 259 260 262 263 264 265 266 268 269 270\n",
      " 271 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289\n",
      " 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 310\n",
      " 311 312 313 315 316 318 320 321 322 323 324 326 327 328 329 330 333 334\n",
      " 335 336 337 338 340 341 342 343 344 346 347 348 354 355 357 358 359 362\n",
      " 363 365 366 368 369 370 371 372 374 375 376 377 378 379 380 381 382 384\n",
      " 385 387 388 389 390 391 392 393 394 395 396 398 399 400 401 402 403 404\n",
      " 405 406 407 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424\n",
      " 425 426 427 428 429 430 431 432 433 434 435 436 437 440 443 444 445 446\n",
      " 449 450 451 452 453 454 455 457 458 459 460 461 462 463 464 465 466 469\n",
      " 470 471 473 474 476 477 478 479 480 482 483 485 486 487 489 492 493 494\n",
      " 495 496 497 498 499 501 502 505] TEST: [  3   6   9  11  13  14  18  21  23  25  32  34  35  38  41  45  47  49\n",
      "  50  58  63  64  65  70  84  89  96 101 111 115 123 130 134 140 142 144\n",
      " 156 160 161 163 172 175 191 193 197 205 215 223 230 236 242 246 247 261\n",
      " 267 272 290 308 309 314 317 319 325 331 332 339 345 349 350 351 352 353\n",
      " 356 360 361 364 367 373 383 386 397 408 409 438 439 441 442 447 448 456\n",
      " 467 468 472 475 481 484 488 490 491 500 503 504]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17  18\n",
      "  19  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37\n",
      "  38  39  41  42  43  44  45  46  47  48  49  50  55  56  58  59  60  61\n",
      "  62  63  64  65  66  68  69  70  71  74  76  77  78  80  81  82  84  85\n",
      "  87  88  89  90  91  92  93  94  96  97  99 100 101 102 105 106 107 108\n",
      " 109 111 112 113 114 115 116 117 119 121 123 124 125 126 127 130 131 132\n",
      " 134 136 137 140 141 142 144 145 147 148 149 151 152 153 154 156 157 158\n",
      " 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176\n",
      " 178 179 181 182 184 187 188 189 190 191 192 193 194 195 196 197 199 200\n",
      " 201 202 203 205 206 207 208 209 210 211 212 215 216 218 221 223 224 229\n",
      " 230 231 233 234 235 236 237 239 240 241 242 244 245 246 247 248 249 251\n",
      " 252 253 254 255 256 258 259 260 261 262 263 264 265 266 267 268 270 271\n",
      " 272 273 275 276 277 279 280 281 283 285 286 287 288 290 291 292 294 295\n",
      " 296 297 298 299 301 302 303 305 306 307 308 309 313 314 315 316 317 319\n",
      " 320 321 322 323 324 325 327 330 331 332 333 335 337 338 339 340 341 342\n",
      " 343 344 345 346 348 349 350 351 352 353 355 356 358 359 360 361 363 364\n",
      " 365 366 367 368 369 370 371 372 373 375 376 378 379 382 383 384 385 386\n",
      " 388 389 390 391 392 393 394 395 396 397 398 399 401 402 403 404 405 407\n",
      " 408 409 410 411 412 413 414 415 416 417 418 419 421 422 423 424 426 428\n",
      " 429 430 431 434 435 436 437 438 439 441 442 444 445 446 447 448 450 452\n",
      " 454 456 457 458 459 460 461 462 463 465 466 467 468 469 470 471 472 474\n",
      " 475 477 478 480 481 482 483 484 486 487 488 489 490 491 492 493 494 495\n",
      " 496 498 499 500 501 502 503 504 505] TEST: [ 15  26  40  51  52  53  54  57  67  72  73  75  79  83  86  95  98 103\n",
      " 104 110 118 120 122 128 129 133 135 138 139 143 146 150 155 177 180 183\n",
      " 185 186 198 204 213 214 217 219 220 222 225 226 227 228 232 238 243 250\n",
      " 257 269 274 278 282 284 289 293 300 304 310 311 312 318 326 328 329 334\n",
      " 336 347 354 357 362 374 377 380 381 387 400 406 420 425 427 432 433 440\n",
      " 443 449 451 453 455 464 473 476 479 485 497]\n",
      "TRAIN: [  0   1   3   4   6   7   9  10  11  12  13  14  15  17  18  19  20  21\n",
      "  22  23  24  25  26  27  28  29  31  32  33  34  35  36  37  38  40  41\n",
      "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  57  58  59  60\n",
      "  61  62  63  64  65  67  68  69  70  72  73  74  75  76  77  78  79  81\n",
      "  82  83  84  85  86  87  89  92  93  94  95  96  97  98 100 101 102 103\n",
      " 104 105 106 107 108 109 110 111 115 116 117 118 120 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 150 151 152 153 155 156 158 159 160 161 162 163 164 166 168\n",
      " 169 170 171 172 173 175 177 178 179 180 181 182 183 185 186 189 191 193\n",
      " 197 198 199 201 202 203 204 205 206 207 208 211 213 214 215 217 219 220\n",
      " 221 222 223 224 225 226 227 228 229 230 231 232 235 236 238 239 241 242\n",
      " 243 245 246 247 250 251 252 254 255 256 257 260 261 262 265 266 267 268\n",
      " 269 270 271 272 273 274 276 278 279 280 281 282 283 284 285 286 287 289\n",
      " 290 291 292 293 298 299 300 301 302 303 304 305 308 309 310 311 312 313\n",
      " 314 315 317 318 319 320 323 324 325 326 327 328 329 331 332 333 334 335\n",
      " 336 337 338 339 342 343 345 346 347 348 349 350 351 352 353 354 355 356\n",
      " 357 358 359 360 361 362 363 364 366 367 368 369 370 371 373 374 375 376\n",
      " 377 378 380 381 383 384 386 387 389 391 392 393 394 395 397 398 400 401\n",
      " 402 404 405 406 407 408 409 410 411 414 415 416 418 419 420 423 424 425\n",
      " 426 427 428 430 431 432 433 434 435 436 438 439 440 441 442 443 444 446\n",
      " 447 448 449 451 452 453 455 456 458 459 461 462 463 464 465 466 467 468\n",
      " 469 472 473 475 476 477 478 479 481 482 483 484 485 486 487 488 490 491\n",
      " 493 496 497 498 500 502 503 504 505] TEST: [  2   5   8  16  30  39  56  66  71  80  88  90  91  99 112 113 114 119\n",
      " 121 148 149 154 157 165 167 174 176 184 187 188 190 192 194 195 196 200\n",
      " 209 210 212 216 218 233 234 237 240 244 248 249 253 258 259 263 264 275\n",
      " 277 288 294 295 296 297 306 307 316 321 322 330 340 341 344 365 372 379\n",
      " 382 385 388 390 396 399 403 412 413 417 421 422 429 437 445 450 454 457\n",
      " 460 470 471 474 480 489 492 494 495 499 501]\n",
      "TRAIN: [  1   2   3   5   6   7   8   9  10  11  13  14  15  16  18  19  21  22\n",
      "  23  24  25  26  29  30  32  33  34  35  36  38  39  40  41  43  44  45\n",
      "  46  47  48  49  50  51  52  53  54  56  57  58  59  63  64  65  66  67\n",
      "  68  70  71  72  73  75  76  77  78  79  80  83  84  85  86  88  89  90\n",
      "  91  92  94  95  96  98  99 101 102 103 104 108 109 110 111 112 113 114\n",
      " 115 117 118 119 120 121 122 123 124 126 128 129 130 132 133 134 135 136\n",
      " 138 139 140 141 142 143 144 146 147 148 149 150 152 153 154 155 156 157\n",
      " 159 160 161 163 165 166 167 172 174 175 176 177 179 180 181 183 184 185\n",
      " 186 187 188 190 191 192 193 194 195 196 197 198 199 200 204 205 206 208\n",
      " 209 210 212 213 214 215 216 217 218 219 220 221 222 223 225 226 227 228\n",
      " 229 230 232 233 234 235 236 237 238 240 242 243 244 245 246 247 248 249\n",
      " 250 253 255 256 257 258 259 260 261 263 264 266 267 268 269 271 272 274\n",
      " 275 277 278 279 282 283 284 285 286 287 288 289 290 292 293 294 295 296\n",
      " 297 299 300 301 302 304 305 306 307 308 309 310 311 312 314 315 316 317\n",
      " 318 319 320 321 322 325 326 327 328 329 330 331 332 334 336 337 338 339\n",
      " 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357\n",
      " 358 360 361 362 364 365 366 367 369 370 372 373 374 375 376 377 378 379\n",
      " 380 381 382 383 385 386 387 388 390 391 392 393 396 397 399 400 401 403\n",
      " 405 406 408 409 412 413 417 418 419 420 421 422 423 425 427 429 430 431\n",
      " 432 433 435 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451\n",
      " 453 454 455 456 457 458 459 460 462 463 464 465 466 467 468 470 471 472\n",
      " 473 474 475 476 477 479 480 481 484 485 486 487 488 489 490 491 492 494\n",
      " 495 496 497 499 500 501 502 503 504] TEST: [  0   4  12  17  20  27  28  31  37  42  55  60  61  62  69  74  81  82\n",
      "  87  93  97 100 105 106 107 116 125 127 131 137 145 151 158 162 164 168\n",
      " 169 170 171 173 178 182 189 201 202 203 207 211 224 231 239 241 251 252\n",
      " 254 262 265 270 273 276 280 281 291 298 303 313 323 324 333 335 359 363\n",
      " 368 371 384 389 394 395 398 402 404 407 410 411 414 415 416 424 426 428\n",
      " 434 436 452 461 469 478 482 483 493 498 505]\n",
      "TRAIN: [  0   2   3   4   5   6   8   9  11  12  13  14  15  16  17  18  20  21\n",
      "  23  25  26  27  28  30  31  32  34  35  37  38  39  40  41  42  45  47\n",
      "  49  50  51  52  53  54  55  56  57  58  60  61  62  63  64  65  66  67\n",
      "  69  70  71  72  73  74  75  79  80  81  82  83  84  86  87  88  89  90\n",
      "  91  93  95  96  97  98  99 100 101 103 104 105 106 107 110 111 112 113\n",
      " 114 115 116 118 119 120 121 122 123 125 127 128 129 130 131 133 134 135\n",
      " 137 138 139 140 142 143 144 145 146 148 149 150 151 154 155 156 157 158\n",
      " 160 161 162 163 164 165 167 168 169 170 171 172 173 174 175 176 177 178\n",
      " 180 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 200 201 202 203 204 205 207 209 210 211 212 213 214 215 216 217 218 219\n",
      " 220 222 223 224 225 226 227 228 230 231 232 233 234 236 237 238 239 240\n",
      " 241 242 243 244 246 247 248 249 250 251 252 253 254 257 258 259 261 262\n",
      " 263 264 265 267 269 270 272 273 274 275 276 277 278 280 281 282 284 288\n",
      " 289 290 291 293 294 295 296 297 298 300 303 304 306 307 308 309 310 311\n",
      " 312 313 314 316 317 318 319 321 322 323 324 325 326 328 329 330 331 332\n",
      " 333 334 335 336 339 340 341 344 345 347 349 350 351 352 353 354 356 357\n",
      " 359 360 361 362 363 364 365 367 368 371 372 373 374 377 379 380 381 382\n",
      " 383 384 385 386 387 388 389 390 394 395 396 397 398 399 400 402 403 404\n",
      " 406 407 408 409 410 411 412 413 414 415 416 417 420 421 422 424 425 426\n",
      " 427 428 429 432 433 434 436 437 438 439 440 441 442 443 445 447 448 449\n",
      " 450 451 452 453 454 455 456 457 460 461 464 467 468 469 470 471 472 473\n",
      " 474 475 476 478 479 480 481 482 483 484 485 488 489 490 491 492 493 494\n",
      " 495 497 498 499 500 501 503 504 505] TEST: [  1   7  10  19  22  24  29  33  36  43  44  46  48  59  68  76  77  78\n",
      "  85  92  94 102 108 109 117 124 126 132 136 141 147 152 153 159 166 179\n",
      " 181 199 206 208 221 229 235 245 255 256 260 266 268 271 279 283 285 286\n",
      " 287 292 299 301 302 305 315 320 327 337 338 342 343 346 348 355 358 366\n",
      " 369 370 375 376 378 391 392 393 401 405 418 419 423 430 431 435 444 446\n",
      " 458 459 462 463 465 466 477 486 487 496 502]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 We can use the builtin Package cross_val_score to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11519645 -0.36833356 -0.82039383 -0.84205337 -0.43112767]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "model = Lasso(alpha=0.15)                    # Here alpha is like lambda in our slide\n",
    "print(cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "# Output the cross validation score for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12284232 -0.31074328 -0.47458661 -0.79483972 -0.2517076 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "model = Ridge(alpha=0.15*(2*len(y)))                    # Here alpha is like lambda in our slide\n",
    "print(cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "# Output the cross validation score for each fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Grid Search and CV search for the best lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenghua/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/chenghua/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:9: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# First step, transformation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "y = target\n",
    "df_dummy = pd.get_dummies(df, columns = ['CHAS'],drop_first = True) # Change categorical to one-hot\n",
    "X = df_dummy\n",
    "scaler_x, scaler_y = StandardScaler(), StandardScaler()\n",
    "scaler_x.fit(X)\n",
    "scaler_y.fit(y)\n",
    "X = scaler_x.transform(X)\n",
    "y = scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenghua/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAF6CAYAAAATeYHoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xt0XWd95vHnOTcdyZIcO7Zkkzg4\nLiYJCeCAYBpKuCQOmBZwwtAUmq5JWmhK6XVWmdV0ZWaYGaZt2g69MJ1CTQqkNJQGmkAIISExl3BJ\nZ3Bu4MQkIRcSB19kh8QXRbIuv/njHNmydSRLis7Z2nt/P2tpad90zk/bkh6/737fvR0RAgAA2VdI\nugAAANAahD4AADlB6AMAkBOEPgAAOUHoAwCQE4Q+AAA5QegDAJAThD4AADlB6AMAkBOlpAtohmXL\nlsXq1auTLgMAgJa466679kTE8uMdl8nQX716tbZs2ZJ0GQAAtITtH8/kOLr3AQDIicRC3/ZS27fZ\nfrj+eck0x3bb3m77b1tZIwAAWZJkS/8KSZsjYq2kzfX1qXxI0h0tqQoAgIxKMvQ3SrqmvnyNpAsb\nHWT7lZJ6JX21RXUBAJBJSYZ+b0TsqC/vVC3Yj2K7IOnDkj5wvBezfbntLba39Pf3z2+lAABkQFNH\n79u+XdKKBruunLgSEWE7Ghz3fkk3R8R229O+V0RskrRJkvr6+hq9FgAAudbU0I+I9VPts73L9sqI\n2GF7paTdDQ47R9K5tt8vqVNSxfaBiJju+j8AAGggyXn6N0q6VNJV9c9fPPaAiLhkfNn2ZZL6CHwA\nAOYmyWv6V0m6wPbDktbX12W7z/bVCdYFAEAmOSJ7l7/7+vqCO/IBAPLC9l0R0Xe847gjHwAAOUHo\nAykyODyqgUMjSZcBIKUy+cCdvXv36lOf+tRR284880y96lWv0vDwsK699tpJX7Nu3TqtW7dOAwMD\nuu666ybt7+vr01lnnaVnn31WN9xww6T955xzjk477TTt2bNHN91006T9r3vd67RmzRrt3LlTt9xy\ny6T9559/vlatWqUnn3xSmzdvnrR/w4YNWrFihR599FHdccfkmxO+9a1v1bJly/Tggw/qzjvvnLT/\noosu0uLFi7V169aGDyO6+OKL1dHRoXvvvVf33nvvpP2XXHKJyuWyvve97+n++++ftP+yyy6TJH33\nu9/VQw89dNS+crmsSy6pjcn85je/qccee+yo/R0dHbr44oslSbfffru2b99+1P7u7m694x3vkCTd\ncsst2rlz51H7TzzxRL3tbW+TJH3pS1/S3r17j9q/YsUKbdiwQZJ0/fXXa9++fUftP/nkk7V+fW2i\nyXXXXaeBgYGj9p966ql6/etfL0m69tprNTw8fNT+F7/4xXrNa14jSZN+7qTZ/+wdGhnTgaERHRga\n0cGhET3uFXpkZIlGBg/qZwuPyLZOXtKuk05ol8TPHj978/ezdyz+7qXnZ2+mMhn6wEI3OhY6ODSi\n54ZHdfMPduiTPwztevpZ9fT/VIdGxyRJBVsdlaJOXtaun+lZoS4Pa+zxHRoYGtGTTw/owOCIfqan\nM+HvBECaMJAPaIGfPPOc7nioX/c88YzuffIZPbR7v8Z/9UoFa8Xiql5wQrtWLenQS0/q1stXnaAz\nVnarWi5Oeq2I0D/e+WP9zy8/oBWLq/roJa/UWSctbvF3BGAhmelAPkIfaIKI0Nan9um2bbt0+wO7\n9MCOWrfu4vay1q06QWefcoLWrTpBp63oUk9XVcXC9HecbOSeJ36q37r2bu05eEh/8c6XaeO6k+b7\n2wCQEjMNfbr3gXlycGhE3/7RHn39h7v19Qd3a9e+IRUsvfKFS/RHbzld553eoxf1dOp4t5SeqbNP\nWaKbfvdcve/Td+kP//X7eulJi7VmOd39AKZG6ANzEBHatW9IP3jqWf3gqWd1949/qv/32NM6NDqm\nrraSzn3xMr3xtB6dd3qPTuxsa1odSxdV9L9/+Wy96a/u0Ac+d58+977XzKnXAEA+EPrAFIZGRvX0\nwUPq3z+kJ59+Tk/+dEBPPD2gJ58e0LYd+7XnwJAkqWBpbU+XLvu51XrDacv1qtVLVS62bjZsb3dV\n//3tZ+r3/+VeXf2tR/Ubr/+Zlr03gHTJZOgPjYzp0f4DSZfRckmPzpg8PGRyRePHxDHrtW1xeFtE\nbf3w8RPWx6J2ZERoLKSxsfrnCI2MhcbGap9Hx8Y0PBoaGRvT8EhoeGxMQ8NjGhoZ0+DwqAZHRjV4\naFQHhkZ1sD5F7sDQiJ4ZOKS9Bw5p/9Dk+fBLOspatbRDr3vxMr30pMV66UmL9ZIXdKujkuyv0sZ1\nL9BXtu7Qh297SOed3qO1vV2J1gNgYcrkQL62lWtj5aV/nXQZWOAqpYKqpYK6qmUtaiuqs62kRW0l\nLemoaOmiik5cVNGJnW1a1lnRyUs6tGppu7qq5aTLntKeA0N601/doZOXtOv633yNSi3sbQCQrFwP\n5Fu1pEMffte6pMvIpWMHqTW6ujx+iOt7J36JjzrGsmvb7NrRhUL96ywVbRVsFVz7wlKhoGJBKhYK\nKtoqFqxKySoVCiqXCioXrLZSUW3lgirFggoZu/a9rLNNH9p4ln7rM3fro994RL9z/tqkSwKwwGQy\n9E/oKDN9Cbn0Cy9bqa9sXamPfO1hnfvi5Vq36oSkSwKwgND/B2TMhzaepd7uqn7j01u0e99g0uUA\nWEAIfSBjliyq6OP/oU/7B0d0+afv0uDwaNIlAVggCH0gg85Y2a2/vPjluvfJZ3TlDVuVxQG7AGaP\n0AcyasNZK/V756/Vv969XZ/4zuNJlwNgASD0gQz7vfPX6s1n9uqPv/yAvv3wnqTLAZAwQh/IsELB\n+suL12n1skX6k5u3JV0OgIQR+kDGLWor6ZdffYoe2LFPT+wdSLocAAki9IEcePOZKyRJt9y/I+FK\nACSJ0AdyYNXSDp11Ure+snVn0qUASBChD+TEW85aqXueeEY7n+WGPUBeEfpATox38d96P619IK8I\nfSAnXtTTqRf1dOoWuviB3CL0gRx5y1kr9H8f26u9B4aSLgVAAgh9IEfefOYKjYV0+7ZdSZcCIAGE\nPpAjZ76gW6uWtjOKH8gpQh/IEdvacOYKfedHe7RvcDjpcgC0GKEP5MyGs1ZoeDT0tW27ky4FQIsR\n+kDOnL1qiXq62hjFD+QQoQ/kTKFgvfnMFfrGQ7s1cGgk6XIAtBChD+TQ+pf0anB4THf/+JmkSwHQ\nQomEvu2ltm+z/XD985Ipjhu1fW/948ZW1wlk1Wm9XZKkR/ccSLgSAK2UVEv/CkmbI2KtpM319Uae\ni4h19Y+3t648INt6u9u0qFLUo/0Hky4FQAslFfobJV1TX75G0oUJ1QHkkm2dunyRHumnpQ/kSVKh\n3xsR4w/23impd4rjqra32P4329P+x8D25fVjt/T3989rsUAWrVnWSUsfyJlSs17Y9u2SVjTYdeXE\nlYgI2zHFy7wwIp6yvUbS12z/ICIeaXRgRGyStEmS+vr6pno9AHVrli/Sl77/Ew0Oj6paLiZdDoAW\naFroR8T6qfbZ3mV7ZUTssL1SUsO7hETEU/XPj9r+hqSzJTUMfQCzs2Z5pyKkx/Yc1Bkru5MuB0AL\nJNW9f6OkS+vLl0r64rEH2F5iu62+vEzSz0l6oGUVAhm3ZtkiSaKLH8iRpEL/KkkX2H5Y0vr6umz3\n2b66fswZkrbYvk/S1yVdFRGEPjBP1iwfD30G8wF50bTu/elExF5J5zfYvkXSe+vL35X00haXBuRG\nR6WklYurenQPLX0gL7gjH5Bja5YvoqUP5AihD+TY+LS9CCa8AHlA6AM5tmb5Iu0fGlH/gaGkSwHQ\nAoQ+kGNrlndKYgQ/kBeEPpBj49P2HmMwH5ALhD6QYyed0K62UoHBfEBOEPpAjhUK1qnLFtG9D+QE\noQ/k3Jrli5irD+QEoQ/k3JplnXri6QEdGhlLuhQATUboAzm3ZvkijY6Fnnh6IOlSADQZoQ/k3JFp\newzmA7KO0Ady7tTxp+1xXR/IPEIfyLnF7WUt66zQ0gdygNAHcPge/ACyjdAHwLQ9ICcIfQBas3yR\nnj54SM8MHEq6FABNROgD0JpltRH8j9DFD2QaoQ9Aa5bXR/AzmA/INEIfgFYt7VCpYK7rAxlH6ANQ\nuVjQKSd20NIHMo7QByBJOmVph37yzGDSZQBoIkIfgCSpt6uqXfsIfSDLCH0AkqTe7jbtOTCkkVGe\ntgdkFaEPQJLU013VWEh7DzJXH8gqQh+AJKm3uypJdPEDGUboA5BU696XpF37hhKuBECzEPoAJNHS\nB/KA0AcgSTpxUUUFS7sJfSCzCH0AkqRSsaBlnW107wMZRugDOKy3u6pd+2npA1lF6AM4rLeblj6Q\nZYQ+gMN6uqtc0wcyLJHQt73U9m22H65/XjLFcafY/qrtbbYfsL26tZUC+dLbVdXeg4d0aIS78gFZ\nlFRL/wpJmyNiraTN9fVG/lHSX0TEGZJeLWl3i+oDcml8rn7/Abr4gSxKKvQ3SrqmvnyNpAuPPcD2\nSySVIuI2SYqIAxEx0LoSgfxhrj6QbUmFfm9E7Kgv75TU2+CYF0t6xvb1tu+x/Re2i1O9oO3LbW+x\nvaW/v78ZNQOZ11Nv6XNdH8imUrNe2PbtklY02HXlxJWICNvR4LiSpHMlnS3pCUn/IukySf/Q6P0i\nYpOkTZLU19fX6PUAHMeRlj7d+0AWNS30I2L9VPts77K9MiJ22F6pxtfqt0u6NyIerX/NFyT9rKYI\nfQDP39KOikoF070PZFRS3fs3Srq0vnyppC82OOZ7kk6wvby+fp6kB1pQG5BbhYLV08VcfSCrkgr9\nqyRdYPthSevr67LdZ/tqSYqIUUkfkLTZ9g8kWdLHE6oXyI2e7qp2c1c+IJOa1r0/nYjYK+n8Btu3\nSHrvhPXbJL2shaUBudfb3abH9hxMugwATcAd+QAcpbe7Svc+kFGEPoCj9HZX9exzwxocHk26FADz\njNAHcJServG5+rT2gawh9AEc5fBcfQbzAZlD6AM4yvhd+ZirD2QPoQ/gKL1dtZY+3ftA9hD6AI5y\nQkdZlWKB7n0ggwh9AEexrZ7uNlr6QAYR+gAmqc3Vp6UPZA2hD2CS3u42Qh/IIEIfwCQ9XVW694EM\nIvQBTNLbXdX+oREdHBpJuhQA84jQBzBJb32u/u79tPaBLCH0AUxy+K58XNcHMoXQBzBJL3flAzKJ\n0AcwSU83d+UDsojQBzBJV1tJ7eUiLX0gYwh9AJPYrs3VZyAfkCmEPoCGergrH5A5hD6Ahnq7q9pN\n6AOZQugDaKi3q0279g0pIpIuBcA8IfQBNNTbXdVzw6Paz135gMwg9AE01DN+Vz66+IHMIPQBNLS8\ni1vxAllD6ANoqLtaliTtH6R7H8gKQh9AQ13VkiTpAKEPZAahD6ChzrZ66DOQD8gMQh9AQ51VQh/I\nGkIfQENtpaIqxQLX9IEMIfQBTKmrWtKBoeGkywAwTwh9AFPqrJZo6QMZQugDmFJnW4nR+0CGJBL6\ntpfavs32w/XPSxoc80bb9074GLR9YRL1AnnV2VbiNrxAhiTV0r9C0uaIWCtpc339KBHx9YhYFxHr\nJJ0naUDSV1tbJpBvXVVa+kCWJBX6GyVdU1++RtLxWvDvlPSViBhoalUAjtLZVmLKHpAhSYV+b0Ts\nqC/vlNR7nOPfJemfm1sSgGN1VcuEPpAhpWa9sO3bJa1osOvKiSsREbanfGC37ZWSXirp1uO83+WS\nLpekU045Zdb1ApisNnp/WBEh20mXA+B5alroR8T6qfbZ3mV7ZUTsqIf67mle6mJJN0TEtJOFI2KT\npE2S1NfXN+V/IgDMXGdbScOjoaGRMVXLxaTLAfA8JdW9f6OkS+vLl0r64jTHvlt07QOJ6OJWvECm\nJBX6V0m6wPbDktbX12W7z/bV4wfZXi1plaRvJlAjkHuHH7rDCH4gE5rWvT+diNgr6fwG27dIeu+E\n9cclndS6ygBMxJP2gGzhjnwApjT+pD1uxQtkA6EPYErd1bIkaf8gD90BsoDQBzAluveBbCH0AUyp\nk9H7QKYQ+gCmNN7S55o+kA2EPoAptZUKKhdNSx/ICEIfwJRs1x66Q0sfyIQpQ9/2X09Y/r1j9n2q\niTUBWEC6qmVG7wMZMV1L/3UTli89Zt/LmlALgAWIx+sC2TFd6HuKZQA5UnvSHqEPZMF0t+Et2F6i\n2n8MxpfHw5/HbQE50dVW0s59g0mXAWAeTBf6iyXdpSNBf/eEfTy6FsiJzmpJB/pp6QNZMGXoR8Tq\nFtYBYIFi9D6QHdON3n+h7cUT1t9o+29s/0fbldaUByBpXdWy9jOQD8iE6QbyXSdpkSTZXifpc5Ke\nkLRO0t81vzQAC0FXtaRDI2MaGhlNuhQAz9N01/TbI+In9eVfkfSJiPiw7YKke5tfGoCF4PBDdwZH\n1NbJGF4gzWY6Ze88SZslKSLGmloRgAWFJ+0B2TFdS/9rtq+TtEPSEklfkyTbKyUdakFtABaA8Sft\nMVcfSL/pQv/3Jf2SpJWSXhsR4/fhXCHpymYXBmBh6KKlD2TGdFP2QtJnG2y/p6kVAVhQxlv6TNsD\n0m/K0Le9X0ffhMf1dav2f4LuJtcGYAHoqpYlSfuHeOgOkHbTde9vVq0r/3pJn42IJ1pTEoCFZOLo\nfQDpNuXo/Yi4UNKbJfVL+rjtb9p+v+2lLasOQOK6xgfycU0fSL3ppuwpIp6NiE9Keoukv5f0PyRd\n1oK6ACwQbaWCSgXT0gcyYLrufdl+jaR3SzpX0rclXRQR32pFYQAWBtu1h+7Q0gdSb7qBfI9Leka1\nEfyXSxqpb3+FJEXE3VN9LYBs4aE7QDZM19J/XLXR+m+W9CYdfYe+UO0ufQByoKta1j5CH0i96ebp\nv6GFdQBYwLraSjrAlD0g9aYdyAcAkrimD2QEoQ/guLimD2QDoQ/guGjpA9lw3NC3vXkm2wBkV1db\niafsARkw3ZS9qqQOSctsL9GR0fvdkk5qQW0AFoiuaklDI2M6NDKmSokOQiCtpvvt/Q1Jd0k6vf55\n/OOLkv72+b6x7aW2b7P9cP3zkimO+3Pb99veZvsjtt3oOADN08njdYFMmO7e+38TEadK+kBErImI\nU+sfL4+I5x36kq6QtDki1qr2cJ8rjj2gfkfAn5P0MklnSXqVpNfPw3sDmIXO+pP2GMwHpNtM+ul2\n2u6SJNv/2fb143fle542SrqmvnyNpAsbHBOSqpIqktoklSXtmof3BjAL4y19Hq8LpNtMQv+/RMR+\n26+VtF7SP0j66Dy8d29E7Kgv75TUe+wBEXGnpK9L2lH/uDUitjV6MduX295ie0t/f/88lAdg3PiT\n9mjpA+k2k9AfrX/+BUmbIuLLqrW8j8v27ba3NvjYOPG4iAjVWvXHfv2LJJ0h6WTVBg+eZ/vcRu8V\nEZsioi8i+pYvXz6T8gDMENf0gWyY9il7dU/Z/ntJF0j6M9ttmuH8/ohYP9U+27tsr4yIHbZXStrd\n4LCLJP1bRByof81XJJ0jiSf9AS3UWW/pM20PSLeZhPfFkm6V9OaIeEbSUkn/aR7e+0ZJl9aXL1Vt\nVsCxnpD0etsl22XVBvE17N4H0Dzj3fv7aekDqXbc0I+IAdVa4a+tbxqR9PA8vPdVki6w/bBqYwWu\nkiTbfbavrh/zeUmPSPqBpPsk3RcRX5qH9wYwC11tjN4HsuC43fu2PyipT9Jpkj6p2gj6f1JtKt2c\nRcReSec32L5F0nvry6Oq3S8AQIKq5YKKBfOkPSDlZtK9f5Gkt0s6KEkR8RNJXc0sCsDCYpuH7gAZ\nMJPQPzRxdL3tRc0tCcBC1NlW4po+kHIzCf3r6qP3T7D965Jul3T1cb4GQMZ0VWnpA2l33Gv6EfG/\nbF8gaZ9q1/X/a0Tc1vTKACwoXVWetAek3UwG8v1ZRPyhpNsabAOQE51tJe05cCjpMgA8DzPp3r+g\nwba3zHchABa2zmqZO/IBKTdlS9/2b0p6v6Q1tr8/YVeXpO80uzAAC0tnG937QNpN173/GUlfkfSn\nOvqxt/sj4ummVgVgwemqlpinD6TclKEfEc9KelbSu1tXDoCFqrOtpMHhMQ2PjqlcnNHjNwAsMPzm\nApgRHq8LpB+hD2BGeLwukH6EPoAZ6eLxukDqEfoAZqRz/El7tPSB1CL0AcxI5/g1fUbwA6lF6AOY\nkfFr+nTvA+lF6AOYEa7pA+lH6AOYkcNT9rimD6QWoQ9gRtrLRRXMPH0gzQh9ADNiW51tJVr6QIoR\n+gBmrKta5po+kGKEPoAZq7X0mbIHpBWhD2DGOqs8XhdIM0IfwIwtaivp4KHRpMsAMEeEPoAZay8X\nNEjoA6lF6AOYsY5KSQPDdO8DaUXoA5ixarmo52jpA6lF6AOYsY4KoQ+kGaEPYMbay0UNDI8qIpIu\nBcAcEPoAZqy9UlSENDQylnQpAOaA0AcwYx2VoiTRxQ+kFKEPYMbay/XQHyb0gTQi9AHMWHu9pT9A\nSx9IpURC3/ZS27fZfrj+eckUx/2Z7a31j19qdZ0Ajna4pU/oA6mUVEv/CkmbI2KtpM319aPY/gVJ\nr5C0TtK/k/QB290trRLAUToqJUl07wNplVTob5R0TX35GkkXNjjmJZLuiIiRiDgo6fuSNrSoPgAN\ntFdqfzIGDnFXPiCNkgr93ojYUV/eKam3wTH3Sdpgu8P2MklvlLRqqhe0fbntLba39Pf3z3/FANRe\nrrX0B2npA6lUatYL275d0ooGu66cuBIRYXvSnT4i4qu2XyXpu5L6Jd0pacq/NBGxSdImSerr6+PO\nIUATdDCQD0i1poV+RKyfap/tXbZXRsQO2ysl7Z7iNf5Y0h/Xv+Yzkh5qSrEAZmR89D7X9IF0Sqp7\n/0ZJl9aXL5X0xWMPsF20fWJ9+WWSXibpqy2rEMAk7dycB0i1prX0j+MqSdfZfo+kH0u6WJJs90l6\nX0S8V1JZ0rdsS9I+Sb8SEYweAhI0PmWP7n0gnRIJ/YjYK+n8Btu3SHpvfXlQtRH8ABaIcrGgctF0\n7wMpxR35AMxKe5nH6wJpRegDmJX2CqEPpBWhD2BWOiolDdC9D6QSoQ9gVqp07wOpRegDmJWOSlHP\nDTORBkgjQh/ArLSXi0zZA1KK0AcwKwzkA9KL0AcwK7XufUIfSCNCH8CsME8fSC9CH8Cs0L0PpBeh\nD2BW2stFDQyPKoInWANpQ+gDmJWOSlGjY6HhUUIfSBtCH8CsVMs8XhdIK0IfwKx0VGoP52QEP5A+\nhD6AWemo1Fr6A4e4Kx+QNoQ+gFk53L1PSx9IHUIfwKyMt/S5pg+kD6EPYFbaD3fvE/pA2hD6AGal\nne59ILUIfQCz0k73PpBahD6AWTl8TZ+WPpA6hD6AWeko1+bpc00fSB9CH8CsVCu1PxuDtPSB1CH0\nAcxKpVhQsWBuzgOkEKEPYFZs1560R/c+kDqEPoBZa68U6d4HUojQBzBrHRVa+kAaEfoAZq29XGSe\nPpBChD6AWWuvFJmnD6QQoQ9g1mjpA+lE6AOYNa7pA+lE6AOYtWqZ7n0gjQh9ALPWUaF7H0ijRELf\n9i/avt/2mO2+aY7bYPtB2z+yfUUrawQwtY5KiTvyASmUVEt/q6R3SLpjqgNsFyX9H0lvkfQSSe+2\n/ZLWlAdgOtVyUYPDY0mXAWCWEgn9iNgWEQ8e57BXS/pRRDwaEYckfVbSxuZXB+B4OipFHRod08go\nwQ+kyUK+pn+SpCcnrG+vb2vI9uW2t9je0t/f3/TigDxrLxclicF8QMo0LfRt3257a4OPprTWI2JT\nRPRFRN/y5cub8RYA6tor9dBnMB+QKqVmvXBErH+eL/GUpFUT1k+ubwOQsPGWPnP1gXRZyN3735O0\n1vaptiuS3iXpxoRrAqDaNX2J7n0gbZKasneR7e2SzpH0Zdu31re/wPbNkhQRI5J+W9KtkrZJui4i\n7k+iXgBHG+/ep6UPpEvTuvenExE3SLqhwfafSPr5Ces3S7q5haUBmIHx7v1BWvpAqizk7n0AC1RH\npdZeoKUPpAuhD2DW2iu1Px1c0wfShdAHMGvt9Zb+c9yKF0gVQh/ArDFlD0gnQh/ArDFlD0gnQh/A\nrLWVCrK5Ix+QNoQ+gFmzrfZykdAHUobQBzAnHZWiBujeB1KF0AcwJ9VyUYO09IFUIfQBzElHpcjo\nfSBlCH0Ac9JeKdG9D6QMoQ9gTtrLBbr3gZQh9AHMSUelpIFh7sgHpAmhD2BOmLIHpA+hD2BO2iuE\nPpA2hD6AOWkvM08fSBtCH8CcdNDSB1KH0AcwJ+2VooZGxjQ6FkmXAmCGCH0AczL+eN1BuviB1CD0\nAczJ+ON1uSsfkB6EPoA5qdLSB1KH0AcwJx2VkiRa+kCaEPoA5qS9UvvzMXCIu/IBaUHoA5iT9nKt\npf8c3ftAahD6AOZkfCAfc/WB9CD0AcxJ+3jo09IHUoPQBzAn4/P0GcgHpAehD2BOxlv6TNkD0oPQ\nBzAn3JwHSB9CH8CcVEuEPpA2hD6AOSkUrGq5QPc+kCKEPoA566iUuDkPkCKJhL7tX7R9v+0x233T\nHPcJ27ttb21lfQBmpr1c1HOHxpIuA8AMJdXS3yrpHZLuOM5xn5K0oenVAJiT9kpRzw3T0gfSopTE\nm0bENkmyfbzj7rC9ugUlAZiDWkufa/pAWmTmmr7ty21vsb2lv78/6XKAXGivFBm9D6RI00Lf9u22\ntzb42NiM94uITRHRFxF9y5cvb8ZbADhGR6XIbXiBFGla935ErG/WawNYGOjeB9IlM937AFqP7n0g\nXZKasneR7e2SzpH0Zdu31re/wPbNE477Z0l3SjrN9nbb70miXgCNtZeL3JwHSJGkRu/fIOmGBtt/\nIunnJ6y/u5V1AZidDlr6QKrQvQ9gztrLtYF8EZF0KQBmgNAHMGftlVpn4eAwd+UD0oDQBzBnRx6v\ny135gDQg9AHMWXu5FvrM1QfSgdAHMGft9ZY+c/WBdCD0AcwZLX0gXQh9AHM2fk3/7h//VGNjjOAH\nFjpCH8Ccvai3U6uWtuu/fekBbfibO/Svd23X8Cgj+YGFKpGb8wDIhp6uqr72B2/Ql7+/Qx/75iP6\ng8/dpw9/9UGdd0aPisd5dDZmm1GkAAAFyklEQVSA1iP0ATwv5WJBF559kjaue4G+8WC/PvbNR3TT\n93ckXRaABgh9APPCtt54eo/eeHpP0qUAueMPzuw4rukDAJAThD4AADlB6AMAkBOEPgAAOUHoAwCQ\nE4Q+AAA5QegDAJAThD4AADlB6AMAkBOEPgAAOUHoAwCQE4Q+AAA5QegDAJATjoika5h3tvdLejDp\nOhaIZZL2JF3EAsB5OIJzcQTn4gjOxRFpPBcvjIjlxzsoq4/WfTAi+pIuYiGwvYVzwXmYiHNxBOfi\nCM7FEVk+F3TvAwCQE4Q+AAA5kdXQ35R0AQsI56KG83AE5+IIzsURnIsjMnsuMjmQDwAATJbVlj4A\nADgGoQ8AQE4Q+gAA5AShDwBATuQi9G2vsf0Ptj8/Ydsi29fY/rjtS5KsLwm2z7X9MdtX2/5u0vUk\nrf7zsMX2W5OuJUmNflfyxPaF9b8J/2L7TUnXkyTbb7D9rfrfiTckXU+SbJ9i+wu2P2H7iqTreT4W\nfOjXT/Ju21uP2b7B9oO2f3S8f4SIeDQi3nPM5ndI+nxE/Lqkt89z2U01T+fkWxHxPkk3SbqmmfU2\n03yci7o/lHRdc6psjSb+rqTabM5LRHyh/jfhfZJ+KYl6m2mWPyMh6YCkqqTtra612WZ5Ll6qWl78\nmqSzW17sfIqIBf0h6XWSXiFp64RtRUmPSFojqSLpPkkvUe0f5qZjPnomfN3nJyz/kaR19eXPJP19\nJnhOrpPUlfT3lOS5kHSBpHdJukzSW5P+nhbIz8XnW13/QjgvE/Z/WNIrkq494Z+RQn1/r6Rrk649\n4XNxoqSvS/qapF9Nuvbn87Hg770fEXfYXn3M5ldL+lFEPCpJtj8raWNE/KmkmXbPbpd0sqR7lYIe\nj4nm65zYPkXSsxGxv4nlNtV8nIt61+Ui1X65n7N9c0SMNbPuZmji70qqzea82N4m6SpJX4mIu1ta\naAvM8mfkgfr+n0pqa1mRLTKbcyFpWNIH61/zeUmfbGWt8ylVYTfBSZKenLC+vb6tIdsn2v6YpLNt\n/1F98/WS/r3tj0r6UtMqbZ1ZnZO69yjFP7zTmNW5iIgrI+L3JX1G0sfTGPjTmI/flSya6rz8jqT1\nkt5p+31JFJaAhufC9jts/72kT0v620Qqa72pfi5ukfS79d+NxxOoa94s+Jb+fIiIvapdo5u47aCk\nX02mooUhIj6YdA0LSUR8KukaktbodyVPIuIjkj6SdB0LQURcr1rjKPciYqukdyZdx3xIa0v/KUmr\nJqyfXN+WZ5yTIzgXR3AuGuO8HMG5OCLz5yKtof89SWttn2q7otogrBsTrilpnJMjOBdHcC4a47wc\nwbk4IvPnYsGHvu1/lnSnpNNsb7f9nogYkfTbkm6VtE3SdRFxf5J1thLn5AjOxRGci8Y4L0dwLo7I\n67ngKXsAAOTEgm/pAwCA+UHoAwCQE4Q+AAA5QegDAJAThD4AADlB6AMAkBOEPoBZsf247WXP9xgA\nrUfoAwCQE4Q+gCnZ/oLtu2zfb/vyY/attv1D29fa3mb787Y7JhzyO7bvtv0D26fXv+bVtu+0fY/t\n79o+raXfEJBzhD6A6fxaRLxSUp9qjxY98Zj9p0n6u4g4Q9I+Se+fsG9PRLxC0kclfaC+7YeSzo2I\nsyX9V0l/0tTqARyF0Acwnd+1fZ+kf1Pt6WNrj9n/ZER8p778T5JeO2Hf+GNZ75K0ur68WNLnbG+V\n9FeSzmxG0QAaI/QBNGT7DZLWSzonIl4u6R5J1WMOO/bhHRPXh+qfRyWV6ssfkvT1iDhL0tsavB6A\nJiL0AUxlsaSfRsRA/Zr8zzY45hTb59SXf1nSt2fwmuPPJ79sXqoEMGOEPoCp3CKpZHubpKtU6+I/\n1oOSfqt+zBLVrt9P588l/ante3Sk9Q+gRXi0LoA5sb1a0k31rnoAKUBLHwCAnKClDwBATtDSBwAg\nJwh9AABygtAHACAnCH0AAHKC0AcAICf+P311MTF3t6JYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.019179102616724848}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = Lasso()\n",
    "alphas = np.logspace(-10, 10, 100)\n",
    "\n",
    "tuned_parameters = [{'alpha': alphas}]\n",
    "n_folds = 5\n",
    "\n",
    "clf = GridSearchCV(model, tuned_parameters, cv=n_folds, refit=False, scoring='neg_mean_squared_error')\n",
    "clf.fit(X, y)\n",
    "scores = clf.cv_results_['mean_test_score']\n",
    "scores_std = clf.cv_results_['std_test_score']\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, scores)\n",
    "plt.ylabel('test MSE')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])\n",
    "plt.show()\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenghua/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAF6CAYAAAATeYHoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFTgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 83.02175681319736}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = Ridge()\n",
    "alphas = np.logspace(-10, 10, 100)\n",
    "\n",
    "tuned_parameters = [{'alpha': alphas}]\n",
    "n_folds = 5\n",
    "\n",
    "clf = GridSearchCV(model, tuned_parameters, cv=n_folds, refit=False, scoring='neg_mean_squared_error')\n",
    "clf.fit(X, y)\n",
    "scores = clf.cv_results_['mean_test_score']\n",
    "scores_std = clf.cv_results_['std_test_score']\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, scores)\n",
    "plt.ylabel('test MSE')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])\n",
    "plt.show()\n",
    "\n",
    "print(clf.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
